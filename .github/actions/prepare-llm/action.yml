name: Prepare Small LLM
description: Download small LLM and launch it
runs:
  using: composite
  steps:
    - name: Download and unpack llama.cpp (Linux)
      if: runner.os == 'Linux'
      shell: bash
      run: |
        curl -OL https://github.com/ggerganov/llama.cpp/releases/download/b3970/llama-b3970-bin-ubuntu-x64.zip
        unzip llama-b3970-bin-ubuntu-x64.zip

    - name: Download and unpack llama.cpp (macOS)
      if: runner.os == 'macOS'
      shell: bash
      run: |
        curl -OL https://github.com/ggerganov/llama.cpp/releases/download/b3970/llama-b3970-bin-macos-arm64.zip
        unzip llama-b3970-bin-macos-arm64.zip

    - name: Launch llama.cpp
      shell: bash
      run: ./build/bin/llama-server -c 4096 --hf-repo lmstudio-community/granite-3.0-1b-a400m-instruct-GGUF --hf-file granite-3.0-1b-a400m-instruct-Q4_K_M.gguf &

    - name: Wait until it is ready
      shell: bash
      run: while ! curl -s 'http://localhost:8080/health' | grep 'ok'; do sleep 1; done
